<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Large Language Models</title>
    <style>
        :root {
            --primary: #00c3ff;
            --secondary: #0054b4;
            --dark: #0a192f;
            --light: #f8f8ff;
            --accent: #64ffda;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }

        body {
            background-color: var(--dark);
            color: var(--light);
            overflow-y: auto;
            background: linear-gradient(135deg, var(--dark) 0%, #020c1b 100%);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            overflow-y: visible;
        }

        header {
            position: relative;
            height: 100vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            text-align: center;
            overflow: visible;
        }

        .title {
            font-size: 5rem;
            font-weight: 800;
            letter-spacing: -1px;
            background: linear-gradient(90deg, var(--primary), var(--accent));
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            margin-bottom: 2rem;
            opacity: 0;
            transform: translateY(30px);
            animation: fadeIn 1s ease-out forwards 0.2s;
        }

        .subtitle {
            font-size: 1.5rem;
            color: var(--light);
            opacity: 0.8;
            max-width: 800px;
            opacity: 0;
            transform: translateY(30px);
            animation: fadeIn 1s ease-out forwards 0.4s;
        }

        .scroll-indicator {
            position: absolute;
            bottom: 2rem;
            left: 50%;
            transform: translateX(-50%);
            width: 30px;
            height: 50px;
            border: 2px solid var(--primary);
            border-radius: 15px;
        }

        .scroll-indicator::before {
            content: '';
            position: absolute;
            top: 8px;
            left: 50%;
            width: 6px;
            height: 6px;
            background-color: var(--primary);
            border-radius: 50%;
            transform: translateX(-50%);
            animation: scrollDown 2s infinite;
        }

        .section {
            padding: 6rem 0;
            opacity: 0;
            transform: translateY(30px);
            transition: opacity 1s ease, transform 1s ease;
        }

        .section.visible {
            opacity: 1;
            transform: translateY(0);
        }

        .section-title {
            font-size: 2.5rem;
            margin-bottom: 3rem;
            position: relative;
            display: inline-block;
        }

        .section-title::after {
            content: '';
            position: absolute;
            bottom: -10px;
            left: 0;
            width: 100px;
            height: 4px;
            background: linear-gradient(90deg, var(--primary), var(--accent));
            border-radius: 2px;
        }

        .content {
            display: grid;
            grid-template-columns: 1fr;
            gap: 2rem;
        }

        .card {
            background: rgba(255, 255, 255, 0.03);
            border-radius: 10px;
            padding: 2rem;
            border: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(5px);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 195, 255, 0.2);
        }

        .card h3 {
            color: var(--primary);
            margin-bottom: 1rem;
            font-size: 1.5rem;
        }

        .card p {
            color: var(--light);
            opacity: 0.8;
            line-height: 1.6;
        }

        .timeline {
            position: relative;
            max-width: 1200px;
            margin: 0 auto;
        }

        .timeline::after {
            content: '';
            position: absolute;
            width: 4px;
            background: linear-gradient(to bottom, var(--primary), var(--accent));
            top: 0;
            bottom: 0;
            left: 50%;
            margin-left: -2px;
            border-radius: 2px;
        }

        .timeline-item {
            padding: 10px 40px;
            position: relative;
            width: 50%;
            box-sizing: border-box;
        }

        .timeline-item:nth-child(odd) {
            left: 0;
        }

        .timeline-item:nth-child(even) {
            left: 50%;
        }

        .timeline-item::after {
            content: '';
            position: absolute;
            width: 20px;
            height: 20px;
            background-color: var(--dark);
            border: 4px solid var(--primary);
            border-radius: 50%;
            top: 15px;
            z-index: 1;
        }

        .timeline-item:nth-child(odd)::after {
            right: -10px;
        }

        .timeline-item:nth-child(even)::after {
            left: -10px;
        }

        .timeline-content {
            padding: 20px;
            background: rgba(255, 255, 255, 0.03);
            border-radius: 10px;
            border: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(5px);
            transition: transform 0.3s ease;
        }

        .timeline-content:hover {
            transform: scale(1.03);
        }

        .timeline-content h3 {
            color: var(--primary);
            margin-bottom: 10px;
        }

        .timeline-content p {
            color: var(--light);
            opacity: 0.8;
            line-height: 1.6;
        }

        .chip {
            display: inline-block;
            margin: 5px;
            padding: 8px 15px;
            background: rgba(100, 255, 218, 0.1);
            border-radius: 20px;
            font-size: 0.9rem;
            color: var(--accent);
            border: 1px solid rgba(100, 255, 218, 0.3);
            transition: all 0.3s ease;
        }

        .chip:hover {
            background: rgba(100, 255, 218, 0.2);
            transform: translateY(-2px);
        }

        .grid-container {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 2rem;
            margin-top: 2rem;
        }

        .neural-network-viz {
            position: relative;
            height: 300px;
            margin: 2rem 0;
            perspective: 800px;
        }

        .node {
            position: absolute;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: var(--primary);
            box-shadow: 0 0 10px var(--primary), 0 0 20px var(--primary);
            animation: pulse 3s infinite;
        }

        .connection {
            position: absolute;
            height: 2px;
            background: linear-gradient(90deg, rgba(0, 195, 255, 0.5), rgba(100, 255, 218, 0.5));
            transform-origin: left center;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes scrollDown {
            0% {
                opacity: 0;
                top: 8px;
            }
            30% {
                opacity: 1;
            }
            60% {
                opacity: 1;
            }
            100% {
                opacity: 0;
                top: 32px;
            }
        }

        @keyframes pulse {
            0% {
                transform: scale(1);
                opacity: 1;
            }
            50% {
                transform: scale(1.2);
                opacity: 0.7;
            }
            100% {
                transform: scale(1);
                opacity: 1;
            }
        }

        @media (max-width: 768px) {
            .title {
                font-size: 3rem;
            }
            .section-title {
                font-size: 2rem;
            }
            .timeline::after {
                left: 31px;
            }
            .timeline-item {
                width: 100%;
                padding-left: 70px;
                padding-right: 25px;
            }
            .timeline-item:nth-child(even) {
                left: 0;
            }
            .timeline-item::after {
                left: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1 class="title">LARGE LANGUAGE MODELS</h1>
            <p class="subtitle">Exploring the Revolutionary Technology Reshaping Artificial Intelligence and Natural Language Processing</p>
            <div class="scroll-indicator"></div>
        </header>

        <section id="introduction" class="section">
            <h2 class="section-title">Introduction</h2>
            <div class="content">
                <div class="card">
                    <h3>Definition of Large Language Models (LLMs)</h3>
                    <p>Large Language Models (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data to understand, generate, and manipulate human language with remarkable accuracy. These sophisticated neural networks, containing billions or even trillions of parameters, can process and generate text that closely resembles human-written content, recognize patterns in language, and perform a wide range of language-related tasks.</p>
                </div>
                <div class="card">
                    <h3>Importance and Relevance in Modern AI and NLP</h3>
                    <p>LLMs represent a paradigm shift in Natural Language Processing (NLP), enabling unprecedented capabilities in language understanding and generation. They have revolutionized how machines interact with human language, powering applications from chatbots and virtual assistants to content creation tools and research aids. Their impact extends across industries, transforming how businesses engage with customers, how researchers process information, and how individuals interact with technology in their daily lives.</p>
                </div>
                <div class="neural-network-viz" id="neural-viz">
                    <!-- Neural network visualization will be generated by JavaScript -->
                </div>
            </div>
        </section>

        <section id="historical" class="section">
            <h2 class="section-title">Historical Background</h2>
            <div class="content">
                <div class="card">
                    <h3>Evolution of Language Models</h3>
                    <p>The journey of language models began with simple statistical approaches and has evolved through rule-based systems, n-gram models, and neural networks to today's sophisticated transformer-based architectures. Each evolutionary step has brought significant improvements in the ability to capture the nuances and complexities of human language, with modern LLMs representing the culmination of decades of research and technological advancement.</p>
                </div>
                <div class="timeline">
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>1950s-1980s</h3>
                            <p>Early rule-based approaches and statistical language models</p>
                        </div>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>1990s-2000s</h3>
                            <p>N-gram models and early neural network applications</p>
                        </div>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>2013</h3>
                            <p>Introduction of Word2Vec and distributed word representations</p>
                        </div>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>2017</h3>
                            <p>Transformer architecture introduced in "Attention Is All You Need" paper</p>
                        </div>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>2018-2019</h3>
                            <p>BERT, GPT-2, and other transformer-based models demonstrate breakthrough performance</p>
                        </div>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>2020-Present</h3>
                            <p>GPT-3, GPT-4, LLaMA, and other advanced LLMs with billions to trillions of parameters</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="core-concepts" class="section">
            <h2 class="section-title">Core Concepts</h2>
            <div class="content">
                <div class="card">
                    <h3>Neural Networks and Deep Learning Basics</h3>
                    <p>LLMs are built upon deep neural networks—complex mathematical systems inspired by the human brain's structure. These networks consist of interconnected nodes (neurons) organized in layers that process and transform input data. Through training on massive text corpora, these networks learn to recognize patterns, extract features, and model the relationships between words and concepts in language.</p>
                    <div class="chips-container">
                        <span class="chip">Feedforward Networks</span>
                        <span class="chip">Backpropagation</span>
                        <span class="chip">Gradient Descent</span>
                        <span class="chip">Activation Functions</span>
                    </div>
                </div>
                <div class="card">
                    <h3>Transformers Architecture</h3>
                    <p>The transformer architecture, introduced in 2017, revolutionized NLP by enabling models to process entire sequences simultaneously rather than sequentially. This parallel processing capability, combined with self-attention mechanisms, allows transformers to capture long-range dependencies and contextual relationships in text more effectively than previous architectures like RNNs or LSTMs. Modern LLMs primarily utilize transformer-based designs with modifications and optimizations to scale to billions of parameters.</p>
                    <div class="chips-container">
                        <span class="chip">Encoder-Decoder</span>
                        <span class="chip">Self-Attention</span>
                        <span class="chip">Positional Encoding</span>
                        <span class="chip">Layer Normalization</span>
                    </div>
                </div>
                <div class="card">
                    <h3>Attention Mechanisms</h3>
                    <p>Attention mechanisms are the key innovation that enables transformers to excel at language tasks. They allow the model to focus on different parts of the input sequence when producing each element of the output. This dynamic weighting of information helps the model determine which words or phrases are most relevant to each other, capturing nuanced relationships regardless of their distance in the text. Multi-head attention further enhances this capability by allowing the model to attend to information from different representation subspaces.</p>
                    <div class="chips-container">
                        <span class="chip">Query-Key-Value (QKV)</span>
                        <span class="chip">Multi-Head Attention</span>
                        <span class="chip">Scaled Dot-Product</span>
                        <span class="chip">Cross-Attention</span>
                    </div>
                </div>
            </div>
        </section>
    </div>

    <script>
        // Animate sections when they enter the viewport
        const sections = document.querySelectorAll('.section');
        
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -100px 0px'
        };
        
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.classList.add('visible');
                }
            });
        }, observerOptions);
        
        sections.forEach(section => {
            observer.observe(section);
        });

        // Generate neural network visualization
        function createNeuralNetworkViz() {
            const container = document.getElementById('neural-viz');
            const width = container.offsetWidth;
            const height = container.offsetHeight;
            
            // Create layers
            const layers = 4;
            const nodesPerLayer = [5, 8, 8, 5];
            
            const nodeElements = [];
            const connections = [];
            
            // Create nodes
            for (let layer = 0; layer < layers; layer++) {
                const layerX = (width / (layers + 1)) * (layer + 1);
                
                for (let i = 0; i < nodesPerLayer[layer]; i++) {
                    const nodeY = (height / (nodesPerLayer[layer] + 1)) * (i + 1);
                    
                    const node = document.createElement('div');
                    node.className = 'node';
                    node.style.left = `${layerX}px`;
                    node.style.top = `${nodeY}px`;
                    node.style.animationDelay = `${Math.random() * 2}s`;
                    
                    container.appendChild(node);
                    nodeElements.push({
                        element: node,
                        x: layerX,
                        y: nodeY,
                        layer: layer
                    });
                }
            }
            
            // Create connections between layers
            for (let i = 0; i < nodeElements.length; i++) {
                const node = nodeElements[i];
                
                // Connect to next layer
                if (node.layer < layers - 1) {
                    const nextLayer = nodeElements.filter(n => n.layer === node.layer + 1);
                    
                    for (let j = 0; j < nextLayer.length; j++) {
                        const targetNode = nextLayer[j];
                        
                        const connection = document.createElement('div');
                        connection.className = 'connection';
                        
                        // Calculate position and dimensions
                        const dx = targetNode.x - node.x;
                        const dy = targetNode.y - node.y;
                        const length = Math.sqrt(dx * dx + dy * dy);
                        const angle = Math.atan2(dy, dx) * 180 / Math.PI;
                        
                        connection.style.width = `${length}px`;
                        connection.style.left = `${node.x}px`;
                        connection.style.top = `${node.y}px`;
                        connection.style.transform = `rotate(${angle}deg)`;
                        connection.style.opacity = Math.random() * 0.5 + 0.5;
                        
                        container.appendChild(connection);
                        connections.push(connection);
                    }
                }
            }
            
            // Animate connections
            setInterval(() => {
                connections.forEach(conn => {
                    conn.style.opacity = Math.random() * 0.5 + 0.3;
                });
            }, 2000);
        }
        
        // Initialize visualization when DOM is loaded
        document.addEventListener('DOMContentLoaded', () => {
            createNeuralNetworkViz();
            
            // Re-create on window resize
            window.addEventListener('resize', () => {
                const container = document.getElementById('neural-viz');
                while (container.firstChild) {
                    container.removeChild(container.firstChild);
                }
                createNeuralNetworkViz();
            });
        });
    </script>
</body>
</html>